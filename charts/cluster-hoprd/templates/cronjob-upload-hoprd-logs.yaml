{{ if .Values.clusterHoprd.logs.upload.enabled }}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: upload-hoprd-logs-sa
  namespace: {{ .Release.Namespace }}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: upload-hoprd-logs-role
  namespace: {{ .Release.Namespace }}
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "patch", "update"]
  - apiGroups: ["apps"]
    resources: ["deployments/scale"]
    verbs: ["patch"]
  - apiGroups: ["hoprnet.org"]
    resources: ["hoprds"]
    verbs: ["get", "list", "patch", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: upload-hoprd-logs-role-binding
  namespace: {{ .Release.Namespace }}
subjects:
  - kind: ServiceAccount
    name: upload-hoprd-logs-sa
    namespace: {{ .Release.Namespace }}
roleRef:
  kind: Role
  name: upload-hoprd-logs-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: argo-workflows-workflow-role-binding
  namespace: {{ .Release.Namespace }}
subjects:
  - kind: ServiceAccount
    name: upload-hoprd-logs-sa
    namespace: {{ .Release.Namespace }}
roleRef:
  kind: Role
  name: argo-workflows-workflow
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: upload-hoprd-logs
  namespace: {{ .Release.Namespace }}
spec:
  schedule: "{{ .Values.clusterHoprd.logs.upload.schedule }}"
  timezone: Europe/Zurich # Adjust timezone if necessary
  concurrencyPolicy: "Replace" # Ensures only one job runs at a time
  startingDeadlineSeconds: 600 # Deadline to start the job in seconds
  ttlSecondsAfterFinished: 86400 # Cleanup completed tasks after 24 hour
  serviceAccountName: argo-workflow
  workflowSpec:
    entrypoint: main
    templates:
      - name: main
        steps:
          - - name: check-corrupted-logs
              template: check-corrupted-logs
          - - name: stop-deployment
              template: stop-deployment
          - - name: backup-logs
              template: backup-logs
          - - name: start-deployment
              template: start-deployment

      # Task 1: Check for corrupted logs and resync if needed
      - name: check-corrupted-logs
        serviceAccountName: upload-hoprd-logs-sa
        container:
          image: alpine/k8s:1.33.4
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail
              corrupted_channels=$(curl -s -X 'GET' 'http://{{ .Values.logs.upload.sourceNode }}-p2p-tcp:3001/api/v4/channels/corrupted' -H 'accept: application/json' -H "X-Auth-Token: $HOPRD_API_TOKEN")
              if [[ $(jq -r '.channelIds | length' <<<"$corrupted_channels") -gt 0 ]]; then
                echo "There are corrupted channels, deleting nodes database to sync from scratch"
                if ! kubectl patch Hoprds --type=json -p="[{'op': 'replace', 'path': '/spec/deleteDatabase', 'value':true}]" {{ .Values.logs.upload.sourceNode }}; then
                  echo "Failed to patch Hoprds resource"
                  exit 1
                fi
                sleep 1200 # wait 20 minutes for the node to resync
                kubectl wait --for=condition=available deployment/{{ .Values.logs.upload.sourceNode }} --timeout=120m
                corrupted_channels=$(curl -s -X 'GET' 'http://{{ .Values.logs.upload.sourceNode }}-p2p-tcp:3001/api/v4/channels/corrupted' -H 'accept: application/json' -H "X-Auth-Token: $HOPRD_API_TOKEN")
                if [[ $(jq -r '.channelIds | length' <<<"$corrupted_channels") -gt 0 ]]; then
                  echo "Channels are still corrupted after resync, aborting log backup. You might consider changing the RPC provider"
                  exit 1
                fi
              else
                echo "No corrupted channels found, proceeding with log upload"
              fi

          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
          envFrom:
            - secretRef:
                name: {{ .Release.Name }}-env-vars

      # Task 2: Stop Deployment
      - name: stop-deployment
        serviceAccountName: upload-hoprd-logs-sa
        container:
          image: alpine/k8s:1.33.4
          command: ["/bin/bash", "-c"]
          args:
            - |
              if ! kubectl scale deployment {{ .Values.clusterHoprd.logs.upload.sourceNode }} --replicas=0; then
                  echo "Failed to stop deployment"
                  exit 0
              fi
              kubectl wait --for=delete pod -l app={{ .Values.logs.upload.sourceNode }} --timeout=5m
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"

      # Task 3: Backup logs
      - name: backup-logs
        serviceAccountName: upload-hoprd-logs-sa
        container:
          image: gcr.io/google.com/cloudsdktool/google-cloud-cli:stable
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -euo pipefail
              echo "Installing xz-utils..."
              apt-get update 2>/dev/null || { echo "Failed to update apt-get"; exit 1; }
              apt-get install -y xz-utils 2>/dev/null || { echo "Failed to install xz-utils"; exit 1; }

              cd /app/hoprd-db/db
              echo "Creating backup archive..."
              if ! tar -cJf /tmp/{{ .Values.clusterHoprd.logs.upload.logsFileName }} hopr_logs.db*; then
                  echo "Failed to create backup archive"
                  exit 1
              else
                  echo "Backup archive created successfully"
              fi


              echo "Authenticating with GCP..."
              if ! gcloud auth activate-service-account --key-file=/app/service-account/gcp-sa-key.json; then
                  echo "Failed to authenticate with GCP"
                  exit 1
              else
                  echo "Authenticated with GCP successfully"
              fi

              echo "Uploading to GCS with verification..."
              if ! gcloud storage cp /tmp/{{ .Values.clusterHoprd.logs.upload.logsFileName }} gs://{{ .Values.clusterHoprd.logs.upload.bucketName }}/{{ .Values.clusterHoprd.logs.upload.logsFileName }}; then
                  echo "Failed to upload backup"
                  exit 1
              else
                  echo "Backup uploaded successfully"
              fi

              echo "Cleaning up..."
              rm -f /tmp/{{ .Values.clusterHoprd.logs.upload.logsFileName }}
          resources:
            requests:
              memory: "128Mi"
              cpu: "100m"
          volumeMounts:
            - name: hoprd-db
              mountPath: /app/hoprd-db
              readOnly: true
            - name: service-account-key
              mountPath: /app/service-account/gcp-sa-key.json
              subPath: privateKey
              readOnly: true
        volumes:
          - name: hoprd-db
            persistentVolumeClaim:
              claimName: {{ .Values.clusterHoprd.logs.upload.sourceNode }}
          - name: service-account-key
            secret:
              secretName: gcp-sa-key
              items:
                - key: privateKey
                  path: privateKey
      # Task 4: Start Deployment
      - name: start-deployment
        serviceAccountName: upload-hoprd-logs-sa
        container:
          image: alpine/k8s:1.33.4
          command: ["/bin/bash", "-c"]
          args:
            - |
              if ! kubectl scale deployment {{ .Values.clusterHoprd.logs.upload.sourceNode }} --replicas=1; then
                  echo "Failed to start deployment"
                  exit 1
              fi
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
    # ttlStrategy:
    #   secondsAfterCompletion: 86400 # Cleanup completed tasks after 24 hour
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 2
{{- end -}}